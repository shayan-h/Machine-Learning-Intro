"""
The following code uses sklearn.linear_model.SGDRegressor where SGD stands
for Stochastic Gradient Descent: the gradient of the loss is estimated each
sample at a time and the model is updated along the way with a decreasing
strength schedule (aka learning rate). This model is used to predict the price
of a select number of houses given 4 features for each house. The model is trained
on m houses where the data is first normalized to speed up the fitting to the
linear regression model.
"""

import numpy as np
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler

# Function to load input and output data (skip to line 116)
def load_house_data():
    X_train = np.array([[1.24e+03, 3.00e+00, 1.00e+00, 6.40e+01],
                        [1.95e+03, 3.00e+00, 2.00e+00, 1.70e+01],
                        [1.72e+03, 3.00e+00, 2.00e+00, 4.20e+01],
                        [1.96e+03, 3.00e+00, 2.00e+00, 1.50e+01],
                        [1.31e+03, 2.00e+00, 1.00e+00, 1.40e+01],
                        [8.64e+02, 2.00e+00, 1.00e+00, 6.60e+01],
                        [1.84e+03, 3.00e+00, 1.00e+00, 1.70e+01],
                        [1.03e+03, 3.00e+00, 1.00e+00, 4.30e+01],
                        [3.19e+03, 4.00e+00, 2.00e+00, 8.70e+01],
                        [7.88e+02, 2.00e+00, 1.00e+00, 8.00e+01],
                        [1.20e+03, 2.00e+00, 2.00e+00, 1.70e+01],
                        [1.56e+03, 2.00e+00, 1.00e+00, 1.80e+01],
                        [1.43e+03, 3.00e+00, 1.00e+00, 2.00e+01],
                        [1.22e+03, 2.00e+00, 1.00e+00, 1.50e+01],
                        [1.09e+03, 2.00e+00, 1.00e+00, 6.40e+01],
                        [8.48e+02, 1.00e+00, 1.00e+00, 1.70e+01],
                        [1.68e+03, 3.00e+00, 2.00e+00, 2.30e+01],
                        [1.77e+03, 3.00e+00, 2.00e+00, 1.80e+01],
                        [1.04e+03, 3.00e+00, 1.00e+00, 4.40e+01],
                        [1.65e+03, 2.00e+00, 1.00e+00, 2.10e+01],
                        [1.09e+03, 2.00e+00, 1.00e+00, 3.50e+01],
                        [1.32e+03, 3.00e+00, 1.00e+00, 1.40e+01],
                        [1.59e+03, 0.00e+00, 1.00e+00, 2.00e+01],
                        [9.72e+02, 2.00e+00, 1.00e+00, 7.30e+01],
                        [1.10e+03, 3.00e+00, 1.00e+00, 3.70e+01],
                        [1.00e+03, 2.00e+00, 1.00e+00, 5.10e+01],
                        [9.04e+02, 3.00e+00, 1.00e+00, 5.50e+01],
                        [1.69e+03, 3.00e+00, 1.00e+00, 1.30e+01],
                        [1.07e+03, 2.00e+00, 1.00e+00, 1.00e+02],
                        [1.42e+03, 3.00e+00, 2.00e+00, 1.90e+01],
                        [1.16e+03, 3.00e+00, 1.00e+00, 5.20e+01],
                        [1.94e+03, 3.00e+00, 2.00e+00, 1.20e+01],
                        [1.22e+03, 2.00e+00, 2.00e+00, 7.40e+01],
                        [2.48e+03, 4.00e+00, 2.00e+00, 1.60e+01],
                        [1.20e+03, 2.00e+00, 1.00e+00, 1.80e+01],
                        [1.84e+03, 3.00e+00, 2.00e+00, 2.00e+01],
                        [1.85e+03, 3.00e+00, 2.00e+00, 5.70e+01],
                        [1.66e+03, 3.00e+00, 2.00e+00, 1.90e+01],
                        [1.10e+03, 2.00e+00, 2.00e+00, 9.70e+01],
                        [1.78e+03, 3.00e+00, 2.00e+00, 2.80e+01],
                        [2.03e+03, 4.00e+00, 2.00e+00, 4.50e+01],
                        [1.78e+03, 4.00e+00, 2.00e+00, 1.07e+02],
                        [1.07e+03, 2.00e+00, 1.00e+00, 1.00e+02],
                        [1.55e+03, 3.00e+00, 1.00e+00, 1.60e+01],
                        [1.95e+03, 3.00e+00, 2.00e+00, 1.60e+01],
                        [1.22e+03, 2.00e+00, 2.00e+00, 1.20e+01],
                        [1.62e+03, 3.00e+00, 1.00e+00, 1.60e+01],
                        [8.16e+02, 2.00e+00, 1.00e+00, 5.80e+01],
                        [1.35e+03, 3.00e+00, 1.00e+00, 2.1e+01],
                        [1.57e+03, 3.00e+00, 1.00e+00, 1.40e+01],
                        [1.49e+03, 3.00e+00, 1.00e+00, 5.70e+01],
                        [1.51e+03, 2.00e+00, 1.00e+00, 1.60e+01],
                        [1.10e+03, 3.00e+00, 1.00e+00, 2.70e+01],
                        [1.76e+03, 3.00e+00, 2.00e+00, 2.40e+01],
                        [1.21e+03, 2.00e+00, 1.00e+00, 1.40e+01],
                        [1.47e+03, 3.00e+00, 2.00e+00, 2.40e+01],
                        [1.77e+03, 3.00e+00, 2.00e+00, 8.40e+01],
                        [1.65e+03, 3.00e+00, 1.00e+00, 1.90e+01],
                        [1.03e+03, 3.00e+00, 1.00e+00, 6.00e+01],
                        [1.12e+03, 2.00e+00, 2.00e+00, 1.60e+01],
                        [1.15e+03, 3.00e+00, 1.00e+00, 6.20e+01],
                        [8.16e+02, 2.00e+00, 1.00e+00, 3.90e+01],
                        [1.04e+03, 3.00e+00, 1.00e+00, 2.50e+01],
                        [1.39e+03, 3.00e+00, 1.00e+00, 6.40e+01],
                        [1.60e+03, 3.00e+00, 2.00e+00, 2.90e+01],
                        [1.22e+03, 3.00e+00, 1.00e+00, 6.30e+01],
                        [1.07e+03, 2.00e+00, 1.00e+00, 1.00e+02],
                        [2.60e+03, 4.00e+00, 2.00e+00, 2.20e+01],
                        [1.43e+03, 3.00e+00, 1.00e+00, 5.90e+01],
                        [2.09e+03, 3.00e+00, 2.00e+00, 2.60e+01],
                        [1.79e+03, 4.00e+00, 2.00e+00, 4.90e+01],
                        [1.48e+03, 3.00e+00, 2.00e+00, 1.60e+01],
                        [1.04e+03, 3.00e+00, 1.00e+00, 2.50e+01],
                        [1.43e+03, 3.00e+00, 1.00e+00, 2.20e+01],
                        [1.16e+03, 3.00e+00, 1.00e+00, 5.30e+01],
                        [1.55e+03, 3.00e+00, 2.00e+00, 1.20e+01],
                        [1.98e+03, 3.00e+00, 2.00e+00, 2.20e+01],
                        [1.06e+03, 3.00e+00, 1.00e+00, 5.30e+01],
                        [1.18e+03, 2.00e+00, 1.00e+00, 9.90e+01],
                        [1.36e+03, 2.00e+00, 1.00e+00, 1.70e+01],
                        [9.60e+02, 3.00e+00, 1.00e+00, 5.1e+01],
                        [1.46e+03, 3.00e+00, 2.00e+00, 1.60e+01],
                        [1.45e+03, 3.00e+00, 2.00e+00, 2.50e+01],
                        [1.21e+03, 2.00e+00, 1.00e+00, 1.50e+01],
                        [1.55e+03, 3.00e+00, 2.00e+00, 1.60e+01],
                        [8.82e+02, 3.00e+00, 1.00e+00, 4.9e+01],
                        [2.03e+03, 4.00e+00, 2.00e+00, 4.50e+01],
                        [1.04e+03, 3.00e+00, 1.00e+00, 6.20e+01],
                        [1.62e+03, 3.00e+00, 1.00e+00, 1.60e+01],
                        [8.03e+02, 2.00e+00, 1.00e+00, 8.00e+01],
                        [1.43e+03, 3.00e+00, 2.00e+00, 2.1e+01],
                        [1.66e+03, 3.00e+00, 2.00e+00, 1.90e+01],
                        [1.21e+03, 3.00e+00, 1.00e+00, 2.45e+01],
                        [1.05e+03, 1.00e+00, 1.05e+00, 6.50e+01],
                        [1.25e+03, 2.00e+00, 1.00e+00, 6.56e+01],
                        [1.15e+03, 3.00e+00, 1.40e+00, 6.60e+01],
                        [1.35e+03, 5.00e+00, 1.00e+00, 6.50e+01],
                        [1.05e+03, 2.00e+00, 1.06e+00, 6.50e+01],
                        [1.05e+03, 1.00e+00, 1.04e+00, 6.55e+01]])
    
    y_train = np.array([300., 509.8, 394., 540., 415., 230., 560., 294., 718.2, 200.,
                   302., 468., 374.2, 388., 282., 311.8, 401., 449.8, 301., 502.,
                   340., 400.28, 572., 264., 304., 298., 219.8, 490.7, 216.96, 368.2,
                   280., 526.87, 237., 562.43, 369.8, 460., 374., 390., 158., 426.,
                   390., 277.77, 216.96, 425.8, 504., 329., 464., 220., 358., 478.,
                   334., 426.98, 290., 463., 390.8, 354., 350., 460., 237., 288.3,
                   282., 249., 304., 332., 351.8, 310., 216.96, 666.34, 330., 480.,
                   330.3, 348., 304., 384., 316., 430.4, 450., 284., 275., 414.,
                   258., 378., 350., 412., 373., 225., 390., 267.4, 464., 174.,
                   340., 430., 440., 216., 329., 388., 390., 356., 257.8])
    return X_train, y_train

# Load the data set
X_train, y_train = load_house_data()
print(len(X_train), " ", (len(y_train)))

# Scale/normalize the training data
scaler = StandardScaler()
X_norm = scaler.fit_transform(X_train)

# Create and fit the regression model
sgdr = SGDRegressor(max_iter=1000)
sgdr.fit(X_norm, y_train)
print(sgdr)
print(f"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}")

# Print w,b (parameters)
b_norm = sgdr.intercept_
w_norm = sgdr.coef_
print(f"model parameters: w: {w_norm}, b:{b_norm}")

# Predit using model
y_pred_sgdr = sgdr.predict(X_norm)
y_pred = np.dot(X_norm, w_norm) + b_norm
print(f"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgdr).all()}")

X_new = np.array([[1.24e+03, 3.00e+00, 1.00e+00, 6.40e+01],
                  [1.95e+03, 3.00e+00, 2.00e+00, 1.70e+01],
                  [1.72e+03, 3.00e+00, 2.00e+00, 4.20e+01],
                  [1.96e+03, 3.00e+00, 2.00e+00, 1.50e+01]])
y_pred_new = sgdr.predict(X_new)
print(f"prediction of new data using sgdr.predict: {y_pred_new}")
